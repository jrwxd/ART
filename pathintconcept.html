<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interactive Explainer: Path Integral Latent Video Synthesis</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    
    <!-- Chosen Palette: Warm Stone & Indigo -->
    <!-- Application Structure Plan: The application is designed as a guided, non-linear journey. It starts with a high-level vision, then uses interactive cards to explain core concepts. The main 3-phase framework is presented in a tabbed interface to prevent overwhelming the user. A key feature is the interactive HTML/CSS workflow diagram where each step is clickable, revealing more information. This structure was chosen to transform a dense technical document into an explorable experience, allowing users to choose their own depth of exploration from a guided path, enhancing usability and comprehension over a simple linear page. -->
    <!-- Visualization & Content Choices: 
        - Report Info: Core Concepts (Sec 2) -> Goal: Make abstract ideas tangible -> Viz/Method: Interactive cards with simple canvas animations -> Interaction: Click to activate animation -> Justification: Visual metaphors are more intuitive than text alone.
        - Report Info: Action Function Components (Sec 3.3) -> Goal: Demonstrate trade-offs in path weighting -> Viz/Method: Bar chart (Chart.js) paired with sliders -> Interaction: User adjusts sliders to see chart and descriptions update in real-time -> Justification: Turns a static formula into a hands-on, dynamic control panel.
        - Report Info: Workflow Diagram (Sec 6) -> Goal: Explain the end-to-end process interactively -> Viz/Method: HTML/CSS flexbox diagram -> Interaction: Clicking a node opens a modal with details -> Justification: Replaces a static diagram with a layered information experience, reducing initial cognitive load.
        - Report Info: Challenges & Future Directions (Sec 5, 7) -> Goal: Present pros and cons scannably -> Viz/Method: Two-column layout with hover-reveal cards -> Interaction: Hover to see details -> Justification: Encourages quick exploration and comparison.
    -->
    <!-- CONFIRMATION: NO SVG graphics used. NO Mermaid JS used. -->

    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #F5F5F4; /* stone-100 */
            color: #292524; /* stone-800 */
        }
        .nav-link {
            transition: color 0.2s;
        }
        .nav-link:hover {
            color: #4f46e5; /* indigo-600 */
        }
        .concept-card {
            transition: transform 0.3s, box-shadow 0.3s;
        }
        .concept-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 15px -3px rgb(0 0 0 / 0.1), 0 4px 6px -4px rgb(0 0 0 / 0.1);
        }
        .tab-button.active {
            background-color: #4f46e5; /* indigo-600 */
            color: white;
        }
        .workflow-node {
            transition: all 0.3s ease;
            cursor: pointer;
        }
        .workflow-node:hover {
            transform: scale(1.05);
            background-color: #a78bfa; /* violet-400 */
            color: white;
        }
        .workflow-arrow {
            position: relative;
            flex-grow: 1;
            height: 2px;
            background-color: #a5b4fc; /* indigo-300 */
        }
        .workflow-arrow::after {
            content: '';
            position: absolute;
            right: -1px;
            top: -5px;
            border-left: 10px solid #a5b4fc;
            border-top: 6px solid transparent;
            border-bottom: 6px solid transparent;
        }
        .modal-overlay {
            transition: opacity 0.3s ease;
        }
        .modal-content {
            transition: transform 0.3s ease;
        }
        .chart-container {
            position: relative;
            width: 100%;
            max-width: 600px;
            margin-left: auto;
            margin-right: auto;
            height: 350px;
            max-height: 400px;
        }
        @media (max-width: 768px) {
            .chart-container {
                height: 300px;
            }
        }
    </style>
</head>
<body class="antialiased">

    <!-- Header & Navigation -->
    <header class="bg-stone-50/80 backdrop-blur-lg sticky top-0 z-50 border-b border-stone-200">
        <nav class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
            <div class="flex items-center justify-between h-16">
                <div class="flex-shrink-0">
                    <h1 class="text-xl font-bold text-indigo-600">Path Integral Synthesis</h1>
                </div>
                <div class="hidden md:block">
                    <div class="ml-10 flex items-baseline space-x-4">
                        <a href="#vision" class="nav-link px-3 py-2 rounded-md text-sm font-medium text-stone-700">Vision</a>
                        <a href="#concepts" class="nav-link px-3 py-2 rounded-md text-sm font-medium text-stone-700">Core Concepts</a>
                        <a href="#framework" class="nav-link px-3 py-2 rounded-md text-sm font-medium text-stone-700">Framework</a>
                        <a href="#workflow" class="nav-link px-3 py-2 rounded-md text-sm font-medium text-stone-700">Interactive Workflow</a>
                        <a href="#challenges" class="nav-link px-3 py-2 rounded-md text-sm font-medium text-stone-700">Challenges & Future</a>
                    </div>
                </div>
            </div>
        </nav>
    </header>

    <main class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-12">

        <!-- Vision Section -->
        <section id="vision" class="text-center py-16">
            <h2 class="text-4xl font-bold tracking-tight text-stone-900 sm:text-5xl lg:text-6xl">A New Spin on Latent Diffusion</h2>
            <p class="mt-6 max-w-3xl mx-auto text-lg leading-8 text-stone-600">
                This interactive document explores a novel framework for video generation: applying the principles of **path integrals** to the latent space of Stable Diffusion models. We move beyond simple interpolation to explore a multitude of trajectories, aiming for richer, more dynamic, and semantically coherent video transitions.
            </p>
        </section>

        <!-- Core Concepts Section -->
        <section id="concepts" class="py-16">
            <div class="text-center mb-12">
                <h2 class="text-3xl font-bold tracking-tight text-stone-900">Core Concepts Explained</h2>
                <p class="mt-4 max-w-2xl mx-auto text-lg text-stone-600">Click on a card to explore the foundational ideas behind this technique and see a visual metaphor in action.</p>
            </div>
            <div class="grid md:grid-cols-3 gap-8">
                <!-- Card 1: Latent Space -->
                <div id="concept-card-1" class="concept-card bg-white p-6 rounded-lg shadow-md cursor-pointer border-2 border-transparent">
                    <h3 class="text-xl font-semibold mb-2 text-indigo-700">1. Latent Space</h3>
                    <p class="text-stone-600 mb-4">A compressed, lower-dimensional space where complex data (like images) is represented. Smooth movements in this space correspond to smooth visual transitions. Each point is a potential image.</p>
                    <canvas id="canvas-latent" class="w-full h-48 bg-stone-100 rounded"></canvas>
                </div>
                <!-- Card 2: Path Integral -->
                <div id="concept-card-2" class="concept-card bg-white p-6 rounded-lg shadow-md cursor-pointer border-2 border-transparent">
                    <h3 class="text-xl font-semibold mb-2 text-indigo-700">2. Path Integral Adaptation</h3>
                    <p class="text-stone-600 mb-4">Instead of one path from start to end, we sample many possible paths. Each is "weighted" by its quality (e.g., smoothness). The final video is a weighted average of all these potential journeys.</p>
                    <canvas id="canvas-path" class="w-full h-48 bg-stone-100 rounded"></canvas>
                </div>
                <!-- Card 3: Video Synthesis -->
                <div id="concept-card-3" class="concept-card bg-white p-6 rounded-lg shadow-md cursor-pointer border-2 border-transparent">
                    <h3 class="text-xl font-semibold mb-2 text-indigo-700">3. Video Synthesis</h3>
                    <p class="text-stone-600 mb-4">A video is created frame-by-frame. The latent vector for each frame is determined by the "integrated" path. This generates a sequence of images that form a coherent and dynamic video.</p>
                    <canvas id="canvas-video" class="w-full h-48 bg-stone-100 rounded"></canvas>
                </div>
            </div>
        </section>

        <!-- Implementation Framework Section -->
        <section id="framework" class="py-16">
            <div class="text-center mb-12">
                <h2 class="text-3xl font-bold tracking-tight text-stone-900">The 3-Phase Implementation Framework</h2>
                <p class="mt-4 max-w-2xl mx-auto text-lg text-stone-600">This process breaks down the complex task into manageable phases, from defining potential paths to generating the final video.</p>
            </div>
            <!-- Tabs -->
            <div class="mb-4 flex justify-center space-x-2 p-1 bg-stone-200 rounded-lg">
                <button class="tab-button w-full sm:w-auto px-4 py-2 text-sm font-medium rounded-md active" data-tab="phase1">Phase 1: Defining & Sampling Paths</button>
                <button class="tab-button w-full sm:w-auto px-4 py-2 text-sm font-medium rounded-md" data-tab="phase2">Phase 2: Integrating Paths</button>
                <button class="tab-button w-full sm:w-auto px-4 py-2 text-sm font-medium rounded-md" data-tab="phase3">Phase 3: Control & Refinement</button>
            </div>
            <!-- Tab Content -->
            <div class="bg-white p-8 rounded-lg shadow-inner">
                <div id="phase1" class="tab-content">
                    <h3 class="text-2xl font-semibold mb-4 text-stone-800">Phase 1: Defining and Sampling Paths</h3>
                    <p class="text-stone-600 mb-6">This is the exploratory phase. The system generates a vast number of potential evolutionary paths between the start and end points in latent space. The diversity and quality of these paths are fundamental to the final video's richness.</p>
                    <div class="grid md:grid-cols-2 gap-6">
                        <div class="bg-stone-50 p-4 rounded-lg">
                            <h4 class="font-semibold text-indigo-700">Path Generation Strategies</h4>
                            <p class="text-sm text-stone-600">Paths are generated using methods like perturbations on a baseline, stochastic processes (like Brownian Bridges), or guided sampling towards intermediate concepts.</p>
                        </div>
                        <div class="bg-stone-50 p-4 rounded-lg">
                            <h4 class="font-semibold text-indigo-700">The "Action" Function</h4>
                            <p class="text-sm text-stone-600">Each path is assigned a score or "Action" based on desirable qualities. This is the heart of creative control. Use the sliders below to see how different factors can be weighted.</p>
                        </div>
                    </div>
                    <div class="mt-8">
                        <div class="chart-container">
                            <canvas id="actionChart"></canvas>
                        </div>
                        <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mt-6 max-w-3xl mx-auto">
                            <div>
                                <label for="smoothness" class="block text-sm font-medium text-stone-700">Smoothness</label>
                                <input id="smoothness" type="range" min="0" max="100" value="70" class="w-full h-2 bg-stone-200 rounded-lg appearance-none cursor-pointer">
                            </div>
                            <div>
                                <label for="semantic" class="block text-sm font-medium text-stone-700">Semantic Coherence</label>
                                <input id="semantic" type="range" min="0" max="100" value="85" class="w-full h-2 bg-stone-200 rounded-lg appearance-none cursor-pointer">
                            </div>
                            <div>
                                <label for="constraint" class="block text-sm font-medium text-stone-700">Constraint Adherence</label>
                                <input id="constraint" type="range" min="0" max="100" value="50" class="w-full h-2 bg-stone-200 rounded-lg appearance-none cursor-pointer">
                            </div>
                            <div>
                                <label for="novelty" class="block text-sm font-medium text-stone-700">Novelty/Exploration</label>
                                <input id="novelty" type="range" min="0" max="100" value="40" class="w-full h-2 bg-stone-200 rounded-lg appearance-none cursor-pointer">
                            </div>
                        </div>
                        <p id="action-explainer" class="text-center mt-4 text-stone-600 max-w-2xl mx-auto"></p>
                    </div>
                </div>
                <div id="phase2" class="tab-content hidden">
                     <h3 class="text-2xl font-semibold mb-4 text-stone-800">Phase 2: Integrating Over Paths</h3>
                    <p class="text-stone-600">Here, we combine the contributions of all sampled paths. For each frame of the video, the final latent state is a weighted average of the states from all paths at that specific time. Paths with a "better" action score contribute more heavily, guiding the final video towards the desired characteristics.</p>
                </div>
                <div id="phase3" class="tab-content hidden">
                     <h3 class="text-2xl font-semibold mb-4 text-stone-800">Phase 3: Control and Refinement</h3>
                    <p class="text-stone-600">This phase is about user interaction and control. By adjusting prompts, setting waypoints, and tuning the action function weights, the user can steer the creative direction of the video. The system is designed for iterative refinement, allowing for a feedback loop between the user and the generative process.</p>
                </div>
            </div>
        </section>

        <!-- Interactive Workflow Section -->
        <section id="workflow" class="py-16">
            <div class="text-center mb-12">
                <h2 class="text-3xl font-bold tracking-tight text-stone-900">Interactive Workflow</h2>
                <p class="mt-4 max-w-2xl mx-auto text-lg text-stone-600">This diagram illustrates the end-to-end process. Click on any step to see a detailed explanation of what happens at that stage.</p>
            </div>
            <div class="w-full overflow-x-auto pb-4">
                <div class="flex flex-col md:flex-row items-center justify-center space-y-4 md:space-y-0 md:space-x-4 min-w-[800px]">
                    <div class="workflow-node bg-indigo-200 text-indigo-800 font-semibold p-4 rounded-lg shadow-md text-center" data-step="1">User Input</div>
                    <div class="workflow-arrow w-16 md:w-auto h-8 md:h-auto"></div>
                    <div class="workflow-node bg-indigo-200 text-indigo-800 font-semibold p-4 rounded-lg shadow-md text-center" data-step="2">Path Sampling Loop</div>
                    <div class="workflow-arrow w-16 md:w-auto h-8 md:h-auto"></div>
                    <div class="workflow-node bg-indigo-200 text-indigo-800 font-semibold p-4 rounded-lg shadow-md text-center" data-step="3">Path Integration</div>
                    <div class="workflow-arrow w-16 md:w-auto h-8 md:h-auto"></div>
                    <div class="workflow-node bg-indigo-200 text-indigo-800 font-semibold p-4 rounded-lg shadow-md text-center" data-step="4">Frame Generation</div>
                    <div class="workflow-arrow w-16 md:w-auto h-8 md:h-auto"></div>
                    <div class="workflow-node bg-green-200 text-green-800 font-semibold p-4 rounded-lg shadow-md text-center" data-step="5">Output Video</div>
                </div>
            </div>
        </section>
        
        <!-- Challenges & Future Section -->
        <section id="challenges" class="py-16">
            <div class="text-center mb-12">
                <h2 class="text-3xl font-bold tracking-tight text-stone-900">Challenges & The Road Ahead</h2>
                <p class="mt-4 max-w-2xl mx-auto text-lg text-stone-600">This approach is ambitious and presents exciting problems to solve. Here's a look at the key challenges and future directions for this research.</p>
            </div>
            <div class="grid md:grid-cols-2 gap-8">
                <div>
                    <h3 class="text-2xl font-semibold mb-4 text-center">Key Challenges</h3>
                    <div class="space-y-4">
                        <div class="bg-white p-4 rounded-lg shadow-md hover:shadow-xl transition-shadow">
                            <h4 class="font-semibold text-rose-600">Computational Cost</h4>
                            <p class="text-sm text-stone-600">Sampling and evaluating thousands of high-dimensional paths is extremely intensive and requires significant GPU resources.</p>
                        </div>
                        <div class="bg-white p-4 rounded-lg shadow-md hover:shadow-xl transition-shadow">
                            <h4 class="font-semibold text-rose-600">Defining the "Optimal" Action</h4>
                            <p class="text-sm text-stone-600">The action function is subjective. Balancing creative exploration with visual coherence is a major research question.</p>
                        </div>
                         <div class="bg-white p-4 rounded-lg shadow-md hover:shadow-xl transition-shadow">
                            <h4 class="font-semibold text-rose-600">Ensuring Visual Quality</h4>
                            <p class="text-sm text-stone-600">Averaging many paths could lead to blurry or nonsensical results if not carefully guided by a strong action function.</p>
                        </div>
                    </div>
                </div>
                <div>
                    <h3 class="text-2xl font-semibold mb-4 text-center">Future Directions</h3>
                     <div class="space-y-4">
                        <div class="bg-white p-4 rounded-lg shadow-md hover:shadow-xl transition-shadow">
                            <h4 class="font-semibold text-emerald-600">Interactive Path Steering</h4>
                            <p class="text-sm text-stone-600">Develop real-time interfaces that allow a user to "steer" the path integration process dynamically.</p>
                        </div>
                        <div class="bg-white p-4 rounded-lg shadow-md hover:shadow-xl transition-shadow">
                            <h4 class="font-semibold text-emerald-600">Learning the Action Function</h4>
                            <p class="text-sm text-stone-600">Use machine learning to train an action function based on user ratings of generated videos, creating a personalized model.</p>
                        </div>
                         <div class="bg-white p-4 rounded-lg shadow-md hover:shadow-xl transition-shadow">
                            <h4 class="font-semibold text-emerald-600">Conditional Integration</h4>
                            <p class="text-sm text-stone-600">Integrate paths conditioned on other data streams, such as an audio track's beat or a text narrative's emotional arc.</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>

    </main>

    <!-- Modal for Workflow Details -->
    <div id="workflow-modal" class="modal-overlay fixed inset-0 bg-black bg-opacity-50 flex items-center justify-center p-4 z-50 hidden opacity-0">
        <div class="modal-content bg-white rounded-lg shadow-xl max-w-lg w-full p-6 transform scale-95">
            <div class="flex justify-between items-start">
                <h3 id="modal-title" class="text-xl font-semibold text-stone-800"></h3>
                <button id="modal-close" class="text-stone-500 hover:text-stone-800">&times;</button>
            </div>
            <p id="modal-body" class="mt-4 text-stone-600"></p>
        </div>
    </div>
    
    <footer class="text-center py-8 border-t border-stone-200">
        <p class="text-stone-500 text-sm">Conceptual document by John | Interactive experience created by Gemini</p>
    </footer>


<script>
document.addEventListener('DOMContentLoaded', () => {

    // --- Data ---
    const workflowDetails = {
        1: { title: 'Step 1: User Input', body: 'The process begins with user-provided parameters: start/end prompts or images, video duration, quality settings, and weights for the action function. These inputs define the creative boundaries for the video generation.' },
        2: { title: 'Step 2: Path Sampling Loop', body: 'The system generates a large number of candidate paths between the start and end points. Each path is a unique potential video evolution. The action for each path is evaluated and stored, creating a pool of weighted possibilities.' },
        3: { title: 'Step 3: Path Integration', body: 'For each frame of the final video, the system calculates a weighted average of the latent vectors from all sampled paths. Paths with a better "action" score contribute more heavily to this average, ensuring the final result is guided by the desired characteristics.' },
        4: { title: 'Step 4: Frame Generation', body: 'The integrated latent vector for each frame is passed to the Stable Diffusion decoder. This converts the abstract latent representation into a concrete pixel image, one frame at a time.' },
        5: { title: 'Step 5: Output Video', body: 'All the generated frames are assembled in sequence into a final video file. The user can then review the output and optionally refine the input parameters for another iteration.' },
    };

    // --- Tab Functionality ---
    const tabButtons = document.querySelectorAll('.tab-button');
    const tabContents = document.querySelectorAll('.tab-content');
    tabButtons.forEach(button => {
        button.addEventListener('click', () => {
            tabButtons.forEach(btn => btn.classList.remove('active'));
            button.classList.add('active');
            const tabId = button.getAttribute('data-tab');
            tabContents.forEach(content => {
                if (content.id === tabId) {
                    content.classList.remove('hidden');
                } else {
                    content.classList.add('hidden');
                }
            });
        });
    });

    // --- Action Chart Functionality ---
    const actionSliders = {
        smoothness: document.getElementById('smoothness'),
        semantic: document.getElementById('semantic'),
        constraint: document.getElementById('constraint'),
        novelty: document.getElementById('novelty'),
    };
    const actionExplainer = document.getElementById('action-explainer');
    const ctx = document.getElementById('actionChart').getContext('2d');
    const chart = new Chart(ctx, {
        type: 'bar',
        data: {
            labels: ['Smoothness', 'Semantics', 'Constraints', 'Novelty'],
            datasets: [{
                label: 'Weighting',
                data: [70, 85, 50, 40],
                backgroundColor: [
                    'rgba(79, 70, 229, 0.6)',
                    'rgba(129, 140, 248, 0.6)',
                    'rgba(167, 139, 250, 0.6)',
                    'rgba(196, 181, 253, 0.6)',
                ],
                borderColor: [
                    'rgba(79, 70, 229, 1)',
                    'rgba(129, 140, 248, 1)',
                    'rgba(167, 139, 250, 1)',
                    'rgba(196, 181, 253, 1)',
                ],
                borderWidth: 1
            }]
        },
        options: {
            responsive: true,
            maintainAspectRatio: false,
            indexAxis: 'y',
            scales: {
                x: {
                    beginAtZero: true,
                    max: 100
                }
            },
            plugins: {
                legend: {
                    display: false
                },
                tooltip: {
                    callbacks: {
                        label: function(context) {
                            return `Weight: ${context.raw}`;
                        }
                    }
                }
            }
        }
    });

    function updateActionChart() {
        const values = [
            actionSliders.smoothness.value,
            actionSliders.semantic.value,
            actionSliders.constraint.value,
            actionSliders.novelty.value
        ];
        chart.data.datasets[0].data = values;
        chart.update();

        const maxVal = Math.max(...values);
        const dominantFactor = chart.data.labels[values.indexOf(maxVal)];
        actionExplainer.textContent = `Current focus is on ${dominantFactor}, guiding the path generation towards its associated qualities.`;
    }

    Object.values(actionSliders).forEach(slider => {
        slider.addEventListener('input', updateActionChart);
    });
    updateActionChart();

    // --- Modal Functionality ---
    const modal = document.getElementById('workflow-modal');
    const modalClose = document.getElementById('modal-close');
    const modalTitle = document.getElementById('modal-title');
    const modalBody = document.getElementById('modal-body');
    const workflowNodes = document.querySelectorAll('.workflow-node');
    
    workflowNodes.forEach(node => {
        node.addEventListener('click', () => {
            const step = node.dataset.step;
            modalTitle.textContent = workflowDetails[step].title;
            modalBody.textContent = workflowDetails[step].body;
            modal.classList.remove('hidden');
            setTimeout(() => {
                modal.classList.remove('opacity-0');
                modal.querySelector('.modal-content').classList.remove('scale-95');
            }, 10);
        });
    });

    function closeModal() {
        modal.classList.add('opacity-0');
        modal.querySelector('.modal-content').classList.add('scale-95');
        setTimeout(() => modal.classList.add('hidden'), 300);
    }
    
    modalClose.addEventListener('click', closeModal);
    modal.addEventListener('click', (e) => {
        if (e.target === modal) {
            closeModal();
        }
    });

    // --- Concept Canvas Animations ---
    const conceptCanvases = {
        latent: { ctx: document.getElementById('canvas-latent').getContext('2d'), id: 1, animFunc: animateLatentSpace },
        path: { ctx: document.getElementById('canvas-path').getContext('2d'), id: 2, animFunc: animatePathIntegral },
        video: { ctx: document.getElementById('canvas-video').getContext('2d'), id: 3, animFunc: animateVideoSynthesis },
    };
    let activeCanvas = null;

    function resizeCanvas(canvas) {
        const rect = canvas.getBoundingClientRect();
        canvas.width = rect.width;
        canvas.height = rect.height;
    }

    Object.values(conceptCanvases).forEach(c => {
        resizeCanvas(c.ctx.canvas);
        const card = document.getElementById(`concept-card-${c.id}`);
        card.addEventListener('click', () => {
            if (activeCanvas) {
                activeCanvas.card.classList.remove('border-indigo-500', 'shadow-xl');
                cancelAnimationFrame(activeCanvas.animationFrame);
            }
            activeCanvas = { ...c, card };
            card.classList.add('border-indigo-500', 'shadow-xl');
            c.animFunc();
        });
    });
    
    window.addEventListener('resize', () => {
         Object.values(conceptCanvases).forEach(c => resizeCanvas(c.ctx.canvas));
    });

    let frame = 0;
    function animateLatentSpace() {
        frame++;
        const ctx = conceptCanvases.latent.ctx;
        const w = ctx.canvas.width;
        const h = ctx.canvas.height;
        ctx.clearRect(0, 0, w, h);
        
        ctx.fillStyle = 'rgba(79, 70, 229, 0.1)';
        ctx.beginPath();
        ctx.ellipse(w/2, h/2, w*0.4, h*0.3, 0, 0, 2*Math.PI);
        ctx.fill();

        const x = w/2 + Math.cos(frame * 0.02) * w * 0.3;
        const y = h/2 + Math.sin(frame * 0.02) * h * 0.2;

        ctx.fillStyle = '#4f46e5';
        ctx.beginPath();
        ctx.arc(x, y, 8, 0, 2 * Math.PI);
        ctx.fill();

        ctx.font = '12px Inter';
        ctx.fillStyle = '#292524';
        ctx.textAlign = 'center';
        ctx.fillText('Point = Image', x, y + 20);

        if (activeCanvas.id === 1) {
            activeCanvas.animationFrame = requestAnimationFrame(animateLatentSpace);
        }
    }

    function animatePathIntegral() {
        frame++;
        const ctx = conceptCanvases.path.ctx;
        const w = ctx.canvas.width;
        const h = ctx.canvas.height;
        ctx.clearRect(0, 0, w, h);

        const start = { x: w * 0.1, y: h * 0.5 };
        const end = { x: w * 0.9, y: h * 0.5 };

        // Draw multiple paths
        ctx.strokeStyle = 'rgba(167, 139, 250, 0.4)';
        ctx.lineWidth = 1;
        for (let i = 0; i < 5; i++) {
            ctx.beginPath();
            ctx.moveTo(start.x, start.y);
            const cp1x = w * 0.3 + Math.sin(frame * 0.01 + i) * 10;
            const cp1y = h * 0.2 + Math.cos(frame * 0.02 + i*2) * (h * 0.2 * i);
            const cp2x = w * 0.7 + Math.cos(frame * 0.01 + i) * 10;
            const cp2y = h * 0.8 - Math.sin(frame * 0.02 + i*2) * (h * 0.2 * i);
            ctx.bezierCurveTo(cp1x, cp1y, cp2x, cp2y, end.x, end.y);
            ctx.stroke();
        }

        // Draw integrated path
        ctx.strokeStyle = '#4f46e5';
        ctx.lineWidth = 3;
        ctx.beginPath();
        ctx.moveTo(start.x, start.y);
        ctx.bezierCurveTo(w * 0.3, h * 0.5 - 20, w * 0.7, h * 0.5 + 20, end.x, end.y);
        ctx.stroke();

        ctx.fillStyle = '#4f46e5';
        ctx.beginPath();
        ctx.arc(start.x, start.y, 6, 0, 2 * Math.PI);
        ctx.arc(end.x, end.y, 6, 0, 2 * Math.PI);
        ctx.fill();

        if (activeCanvas.id === 2) {
            activeCanvas.animationFrame = requestAnimationFrame(animatePathIntegral);
        }
    }

    let videoFrames = [];
    function animateVideoSynthesis() {
        frame++;
        const ctx = conceptCanvases.video.ctx;
        const w = ctx.canvas.width;
        const h = ctx.canvas.height;
        ctx.clearRect(0, 0, w, h);
        
        const numFrames = 10;
        const frameWidth = w / (numFrames + 2);
        
        if (frame % 15 === 0 && videoFrames.length < numFrames) {
            videoFrames.push({
                hue: Math.random() * 60 + 220, // blues and purples
                y: Math.random() * h * 0.2 + h * 0.4
            });
        }
        if (videoFrames.length > numFrames) {
            videoFrames = [];
        }

        ctx.strokeStyle = '#a78bfa';
        ctx.lineWidth = 2;
        ctx.beginPath();
        ctx.moveTo(0, h/2);
        
        videoFrames.forEach((f, i) => {
            const x = (i + 1) * (w / (numFrames + 1));
            ctx.fillStyle = `hsl(${f.hue}, 70%, 60%)`;
            ctx.fillRect(x - frameWidth/2, h/2 - 20, frameWidth, 40);
            ctx.lineTo(x, f.y);
        });
        ctx.stroke();
        
        if (activeCanvas.id === 3) {
            activeCanvas.animationFrame = requestAnimationFrame(animateVideoSynthesis);
        }
    }
});
</script>

</body>
</html>
